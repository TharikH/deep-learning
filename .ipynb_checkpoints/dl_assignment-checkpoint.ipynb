{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TharikH/deep-learning/blob/main/dl_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxC5XjBomovq",
    "outputId": "50f0bffd-fa97-432c-9fe0-0ca91acf5c5f"
   },
   "outputs": [],
   "source": [
    "# in colab uncomment below statement\n",
    "# !pip install wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import copy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uib1izEEoToJ",
    "outputId": "c11588ed-ee88-456b-a446-9a3b8d56ece4"
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "fEFcyAeBo_D6"
   },
   "outputs": [],
   "source": [
    "num_train_samples = X_train.shape[0]\n",
    "num_size = X_train.shape[1] * X_train.shape[2]\n",
    "num_test_samples = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRTo19OU1L3p",
    "outputId": "f098b3cd-da4e-4c3d-ce8d-0c392ea85eef"
   },
   "outputs": [],
   "source": [
    "input_size=num_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdNI9lnQ2oIM"
   },
   "source": [
    "plot 1 sample image for each class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "AZtSRC16y5b5",
    "outputId": "cccbce4f-2590-4ace-e426-8e164330bcde"
   },
   "source": [
    "example_y = []\n",
    "example_x = []\n",
    "plt.figure(figsize=(10,10))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "k=0\n",
    "wandb.login(key = 'c425b887e2c725018a7f3a772582610fa54ef52c')\n",
    "\n",
    "for i in range(num_train_samples):\n",
    "  if Y_train[i] not in example_y:\n",
    "    example_y.append(Y_train[i])\n",
    "    example_x.append(X_train[i])\n",
    "    \n",
    "    wandb.init(project=\"dl-project-final\")\n",
    "    wandb.run.name = f'{class_names[Y_train[i]]}'\n",
    "    wandb.log({\"examples\": [wandb.Image(X_train[i],caption=class_names[Y_train[i]])]})\n",
    "    \n",
    "    plt.subplot(5,5,k+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[Y_train[i]])\n",
    "    k+=1\n",
    "    if k == 10:\n",
    "      break\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "OaupL4eL2QoO"
   },
   "outputs": [],
   "source": [
    "#parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TfjhpkZr-T5I"
   },
   "outputs": [],
   "source": [
    "#class having all required activations\n",
    "\n",
    "class Activations():\n",
    "  '''\n",
    "  It contains all activations required with its derivatives.\n",
    "  call object.activate(name) function to get the corresponding function, then pass vectors/matrix/values on to these fucntions\n",
    "  similarly call object.derivative(name) to get derivative function.\n",
    "  The names are :\n",
    "  -> sigmoid\n",
    "  -> softmax\n",
    "  -> tanh\n",
    "  -> relu\n",
    "  -> identity\n",
    "  \n",
    "  '''\n",
    "  def __init__(self):\n",
    "    self.activation_dict={\n",
    "        \"sigmoid\":self.sigmoid,\n",
    "        \"softmax\":self.softmax,\n",
    "        \"tanh\":self.tanh,\n",
    "        \"relu\":self.relu,\n",
    "        \"identity\":self.identity\n",
    "    }\n",
    "    self.derivative_dict={\n",
    "        \"sigmoid\":self.sigmoidDerivative,\n",
    "        \"softmax\":self.softmaxDerivative,\n",
    "        \"tanh\":self.tanhDerivative,\n",
    "        \"relu\":self.reluDerivative,\n",
    "        \"identity\":self.identity\n",
    "    }\n",
    "\n",
    "  def activate(self, activation_function = \"sigmoid\"):\n",
    "    return self.activation_dict[activation_function]\n",
    "\n",
    "  def derivate(self,activation_function = \"sigmoid\"):\n",
    "    return self.derivative_dict[activation_function]\n",
    "\n",
    "  def sigmoid(self, x):\n",
    "    z = x.copy()\n",
    "    z[x < 0] = np.exp(x[x < 0])/(1 + np.exp(x[x<0]))\n",
    "    z[x >= 0] = 1/(1+np.exp(-x[x >= 0]))\n",
    "    return z\n",
    "\n",
    "  def softmax(self, x):\n",
    "    max_element = np.max(x,axis=0)\n",
    "    z = np.exp(x - max_element)\n",
    "    total = sum(z)\n",
    "    z = z/total\n",
    "    return z\n",
    "  \n",
    "  def tanh(self, x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "  def identity(self,x):\n",
    "    return x\n",
    "\n",
    "  def identityDerivative(self, x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "  def tanhDerivative(self, x):\n",
    "    z = self.tanh(x)\n",
    "    return 1 - z**2\n",
    "\n",
    "  def softmaxDerivative(self,x):\n",
    "    pass\n",
    "  \n",
    "  def sigmoidDerivative(self,x):\n",
    "    z = self.sigmoid(x)\n",
    "    return  z*(1 - z)\n",
    "  \n",
    "  def relu(self,x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "  def reluDerivative(self,x):\n",
    "    z = x.copy()\n",
    "    z[x < 0]=0\n",
    "    z[x > 0]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "znaRtyTqZ-dT"
   },
   "outputs": [],
   "source": [
    "# class having all losses and its derivatives\n",
    "class Loss():\n",
    "  '''\n",
    "  It contains all Loss and its corresponding derivative with last layer (a_L), assuming last layer activation is softmax.\n",
    "  Initialize the object with name of loss.\n",
    "  call object.findLoss() function to get the corresponding function, then pass appropriate parameter\n",
    "  like Y, Y_hat etc.\n",
    "    \n",
    "  The initializations are :\n",
    "    -> cross_entropy\n",
    "    -> mean_squared_error\n",
    "  \n",
    "  '''\n",
    "    \n",
    "    \n",
    "  def __init__(self,loss_name):\n",
    "    self.loss_dict = {\n",
    "        'cross_entropy':self.crossEntropy,\n",
    "        'mean_squared_error':self.mse\n",
    "    }\n",
    "    self.loss_derivative_dict = {\n",
    "        'cross_entropy':self.crossEntropyDerivativeWithL,\n",
    "        'mean_squared_error':self.mseDerivativeWithL\n",
    "    }\n",
    "    self.loss_name = loss_name\n",
    "\n",
    "  def findLoss(self):\n",
    "    return self.loss_dict[self.loss_name]\n",
    "  \n",
    "  def findDerivative(self):\n",
    "    return self.loss_derivative_dict[self.loss_name]\n",
    "\n",
    "  def findOneHotVector(self,Y_hat, Y):\n",
    "    vector = np.zeros(Y_hat.shape)\n",
    "    for i in range(Y_hat.shape[1]):\n",
    "      vector[:,i][Y[i]] = 1\n",
    "    \n",
    "    return vector\n",
    "    \n",
    "  def crossEntropy(self ,Y_hat, Y, weight_decay=0,W=[]):\n",
    "    loss=0\n",
    "    num_samples = Y_hat.shape[1]\n",
    "    for i in range(num_samples):\n",
    "      loss+=np.log(Y_hat[:,i][Y[i]]  if Y_hat[:,i][Y[i]] != 0 else 1e-9)\n",
    "    \n",
    "    decay_loss = 0\n",
    "    for i in range(len(W)):\n",
    "        decay_loss+=np.sum(W[i] ** 2)\n",
    "\n",
    "\n",
    "    return (-loss + (weight_decay * decay_loss)/2)/num_samples\n",
    "\n",
    "  def crossEntropyDerivativeWithL(self,Y_hat, Y):\n",
    "    return -(self.findOneHotVector(Y_hat,Y) - Y_hat)\n",
    "\n",
    "  def mse(self, Y_hat, Y,weight_decay=0,W=[]):\n",
    "    m,n = Y_hat.shape\n",
    "    diff_matrix = (Y_hat - Y)**2\n",
    "    loss = np.sum(diff_matrix)\n",
    "    \n",
    "    decay_loss = 0\n",
    "    for i in range(len(W)):\n",
    "        decay_loss+=np.sum(W[i] ** 2)\n",
    "        \n",
    "    \n",
    "    return (loss/(m*n)) + ((weight_decay * decay_loss)/2)/n\n",
    "\n",
    "  def mseDerivativeWithL(self, Y_hat , Y,weight_decay=0,W=[]):\n",
    "    derivative = np.zeros(Y_hat.shape)\n",
    "    num_rows,num_samples = Y_hat.shape\n",
    "    one_hot_Y = self.findOneHotVector(Y_hat,Y)\n",
    "    for m in range(num_samples):\n",
    "        for j in range(num_rows):\n",
    "            s=0\n",
    "            for i in range(num_rows):\n",
    "                if j == i:\n",
    "                    s+=(Y_hat[i,m] - one_hot_Y[i,m])*Y_hat[i,m]*(1 - Y_hat[j,m])\n",
    "                else:\n",
    "                    s+=(Y_hat[i,m] - one_hot_Y[i,m])*Y_hat[i,m]*(-Y_hat[j,m])\n",
    "                \n",
    "            derivative[j,m] = 2*s\n",
    "    return derivative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tVu-t4PF6tiJ"
   },
   "outputs": [],
   "source": [
    "# class having all the weight initializations\n",
    "class WeightInit():\n",
    "  '''\n",
    "  It contains all weight initializations. initialize the object with name of initialization\n",
    "  call object.initializeWeight() function to get the corresponding function, then pass shapes to get corresponding initializations\n",
    "    \n",
    "  The initializations are :\n",
    "    -> zero\n",
    "    -> random\n",
    "    -> xavier\n",
    "  \n",
    "  '''\n",
    "    \n",
    "  def __init__(self,weight_name):\n",
    "    self.weight_name = weight_name\n",
    "    self.weight_dict={\n",
    "        \"zero\":self.zeroInit,\n",
    "        \"random\":self.randomInit,\n",
    "        \"xavier\":self.xavierInit\n",
    "    }\n",
    "\n",
    "  def initializeWeight(self):\n",
    "    return self.weight_dict[self.weight_name]\n",
    "    \n",
    "  \n",
    "  def zeroInit(self, shape, flag=0):\n",
    "    x = np.zeros(shape)\n",
    "    return x\n",
    "\n",
    "  def randomInit(self, shape, flag=0):\n",
    "    x = np.random.normal(loc=0,scale=1,size=shape)\n",
    "    return x\n",
    "\n",
    "  \n",
    "  def xavierInit(self, shape, flag=0):\n",
    "    x = np.random.randn(*shape) * np.sqrt(2/shape[0]) if flag == 0 else self.zeroInit(shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "V2gS7xaQ50Hd"
   },
   "outputs": [],
   "source": [
    "# classes having all the optimizers\n",
    "class Optimizer():\n",
    "    \n",
    "  '''\n",
    "  It contains all optimizers. initialize the object with name of optimizer\n",
    "  call object.optimize() function to get the corresponding function, then pass appropriate parameter\n",
    "  like training data, validation data, learning rate etc.\n",
    "    \n",
    "  The initializations are :\n",
    "    -> gradient descent\n",
    "    -> stochastic gradient descent\n",
    "    -> momentum\n",
    "    -> nesterov\n",
    "    -> rmsprop\n",
    "    -> adam\n",
    "    -> nadam\n",
    "  \n",
    "  '''\n",
    "\n",
    "  def __init__(self,optimizer_name=\"gd\"):\n",
    "    self.optimizer_name = optimizer_name\n",
    "    self.optimizer_dict={\n",
    "        \"gd\":self.gradient_descent,\n",
    "        \"sgd\":self.gradient_descent,\n",
    "        \"momentum\":self.momentum,\n",
    "        \"nag\":self.nesterov,\n",
    "        \"rmsprop\":self.rmsprop,\n",
    "        \"adam\" : self.adam,\n",
    "        \"nadam\" : self.nadam\n",
    "    }\n",
    "\n",
    "  def optimize(self):\n",
    "    return self.optimizer_dict[self.optimizer_name]\n",
    "    \n",
    "  \n",
    "  def gradient_descent(self, nn, X, Y, X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [], weight_decay = 0):\n",
    "\n",
    "    num_data = X.shape[1]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          nn.W[j] = nn.W[j] - lr * delta_W[nn.num_hidden_layer - j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - lr * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def momentum(self, nn, X, Y, X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    ut_w,ut_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          ut_w[j] = beta*ut_w[j] + delta_W[nn.num_hidden_layer - j]\n",
    "          ut_b[j] = beta*ut_b[j] + delta_b[nn.num_hidden_layer - j] \n",
    "\n",
    "          nn.W[j] = nn.W[j] - lr * ut_w[j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - lr * ut_b[j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def nesterov(self, nn, X, Y, X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    ut_w,ut_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "  \n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "\n",
    "        old_W = copy.deepcopy(nn.W)\n",
    "        old_b = copy.deepcopy(nn.b)\n",
    "        \n",
    "        for k in range(nn.num_hidden_layer + 1):\n",
    "          nn.W[k] = nn.W[k] - beta *  ut_w[k]\n",
    "          nn.b[k] = nn.b[k] - beta *  ut_b[k]\n",
    "\n",
    "        \n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          ut_w[j] = beta*ut_w[j] + delta_W[nn.num_hidden_layer - j]\n",
    "          ut_b[j] = beta*ut_b[j] + delta_b[nn.num_hidden_layer - j] \n",
    "\n",
    "          nn.W[j] = old_W[j] - lr * ut_w[j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = old_b[j] - lr * ut_b[j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "\n",
    "\n",
    "  def rmsprop(self, nn, X, Y,X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9,0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "    epsilon = parameters[1]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          vt_w[j] = beta*vt_w[j] + (1 - beta) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta*vt_b[j] + (1 - beta) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(lr * delta_W[nn.num_hidden_layer - j],np.sqrt(vt_w[j] + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(lr * delta_b[nn.num_hidden_layer - j],np.sqrt(vt_b[j] + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def adam(self, nn, X, Y, X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9,0.99,0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    mt_w,mt_b = nn.initializeWeights(\"zero\")\n",
    "    beta1 = parameters[0]\n",
    "    beta2 = parameters[1]\n",
    "    epsilon = parameters[2]\n",
    "\n",
    "    t=0\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        t+=1\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          mt_w[j] = beta1 * mt_w[j] + (1 - beta1) * delta_W[nn.num_hidden_layer - j]\n",
    "          mt_b[j] = beta1 * mt_b[j] + (1 - beta1) * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "          mt_w_dash = mt_w[j] / (1 - beta1 ** t)\n",
    "          mt_b_dash = mt_b[j] / (1 - beta1 ** t)\n",
    "\n",
    "\n",
    "          vt_w[j] = beta2*vt_w[j] + (1 - beta2) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta2*vt_b[j] + (1 - beta2) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          vt_w_dash = vt_w[j] / (1 - beta2 ** t)\n",
    "          vt_b_dash = vt_b[j] / (1 - beta2 ** t)           \n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(lr * mt_w_dash,np.sqrt(vt_w_dash + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(lr * mt_b_dash,np.sqrt(vt_b_dash + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def nadam(self, nn, X, Y,X_val, Y_val, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9, 0.999, 0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    mt_w,mt_b = nn.initializeWeights(\"zero\")\n",
    "    beta1 = parameters[0]\n",
    "    beta2 = parameters[1]\n",
    "    epsilon = parameters[2]\n",
    "\n",
    "    t=0\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        t+=1\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          mt_w[j] = beta1 * mt_w[j] + (1 - beta1) * delta_W[nn.num_hidden_layer - j]\n",
    "          mt_b[j] = beta1 * mt_b[j] + (1 - beta1) * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "          mt_w_dash = mt_w[j] / (1 - beta1 ** t)\n",
    "          mt_b_dash = mt_b[j] / (1 - beta1 ** t)\n",
    "\n",
    "\n",
    "          vt_w[j] = beta2*vt_w[j] + (1 - beta2) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta2*vt_b[j] + (1 - beta2) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          vt_w_dash = vt_w[j] / (1 - beta2 ** t)\n",
    "          vt_b_dash = vt_b[j] / (1 - beta2 ** t)           \n",
    "\n",
    "          w_update_numerator = lr * (beta1 * mt_w_dash + ((1 - beta1)* delta_W[nn.num_hidden_layer - j]/(1 - beta1 ** t)))\n",
    "          b_update_numerator = lr * (beta1 * mt_b_dash + ((1 - beta1)* delta_b[nn.num_hidden_layer - j]/(1 - beta1 ** t)))\n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(w_update_numerator,np.sqrt(vt_w_dash + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(b_update_numerator,np.sqrt(vt_b_dash + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = nn.loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = nn.loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = nn.loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "5lWfym6w4BKL"
   },
   "outputs": [],
   "source": [
    "# Base class for all neural networks\n",
    "\n",
    "class NeuralNetwork():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def getParameters(self):\n",
    "    pass\n",
    "  def feedforward():\n",
    "    pass\n",
    "  def backpropogation():\n",
    "    pass\n",
    "  def test(self):\n",
    "    pass\n",
    "  def train(self):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "EPQ7zGjg6gEk"
   },
   "outputs": [],
   "source": [
    "# Neural Network for this particular neural network\n",
    "\n",
    "class NN(NeuralNetwork):\n",
    "\n",
    "  '''\n",
    "  It contains the neural network desired for this application.\n",
    "  Hidden layers, its sizes etc are all dynamic and can be set.\n",
    "  \n",
    "  '''\n",
    "  def __init__(self, num_samples = 60000, input_size = 784, output_size = 10, num_hidden_layer = 3, hidden_layer_size=np.array([64,64,64]), data_name = \"Fashion_mnsit\", hidden_layer_activation=\"relu\", output_layer_activation=\"softmax\", weight_name=\"xavier\",loss_name=\"cross_entropy\"):\n",
    "    self.num_samples = num_samples\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.num_hidden_layer = num_hidden_layer\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.W, self.b = self.initializeWeights(weight_name)\n",
    "    self.hidden_layer_activation = hidden_layer_activation\n",
    "    self.output_layer_activation = output_layer_activation\n",
    "    self.loss_name = loss_name\n",
    "    self.activation_function = Activations()\n",
    "    self.activate_hidden = self.activation_function.activate(hidden_layer_activation)\n",
    "    self.activate_hidden_derivative = self.activation_function.derivate(hidden_layer_activation)\n",
    "    self.activate_output = self.activation_function.activate(output_layer_activation)\n",
    "    self.lossFunction = Loss(loss_name)\n",
    "    self.loss = self.lossFunction.findLoss()\n",
    "    self.lossDerivative = self.lossFunction.findDerivative()\n",
    "    self.parameters = {\n",
    "        \"data_name\":data_name,\n",
    "        \"num_samples\":num_samples,\n",
    "        \"input_size\":input_size,\n",
    "        \"output_size\":output_size,\n",
    "        \"num_hidden_layer\":num_hidden_layer,\n",
    "        \"hidden_layer_size\":hidden_layer_size,\n",
    "        \"hidden_layer_activation\":hidden_layer_activation,\n",
    "        \"output_layer_activation\":output_layer_activation,\n",
    "        \"weight_init\":weight_name,\n",
    "        \"loss_name\":loss_name\n",
    "    }\n",
    "\n",
    "  def getParameters(self):\n",
    "    return self.parameters\n",
    "\n",
    "  def initializeWeights(self, weight_name):\n",
    "    W = []\n",
    "    b= []\n",
    "    input_size = self.input_size\n",
    "    weight_init = WeightInit(weight_name).initializeWeight()\n",
    "    for i in range(self.num_hidden_layer):\n",
    "      output_size = self.hidden_layer_size[i]\n",
    "      W.append(weight_init((input_size, output_size ),0))\n",
    "      b.append(weight_init((output_size, 1 ),1))\n",
    "      input_size = output_size\n",
    "    \n",
    "    output_size = self.output_size\n",
    "\n",
    "    W.append(weight_init((input_size, output_size),0))\n",
    "    b.append(weight_init((output_size, 1),1))\n",
    "\n",
    "    return W, b\n",
    "\n",
    "  def calculateAccuracy(self, X, Y):\n",
    "    Y_hat = self.feedforward(X)\n",
    "    size = Y_hat.shape[1]\n",
    "    score=0\n",
    "    for i in range(size):\n",
    "      if(np.argmax(Y_hat[:,i]) ==  Y[i]):\n",
    "          score+=1\n",
    "\n",
    "    return score/size * 100\n",
    "\n",
    "  def feedforward(self, X):\n",
    "    a = self.W[0].T @ X + self.b[0]\n",
    "    hidden_layer_input = self.activate_hidden(a)\n",
    "\n",
    "    for i in range(1,self.num_hidden_layer):\n",
    "      a=self.W[i].T @ hidden_layer_input + self.b[i]\n",
    "      hidden_layer_output=self.activate_hidden(a)\n",
    "      hidden_layer_input = hidden_layer_output\n",
    "\n",
    "    a=self.W[self.num_hidden_layer].T @ hidden_layer_input + self.b[self.num_hidden_layer]\n",
    "    output = self.activate_output(a)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def forwardpropogation(self, X):\n",
    "    a_values=[]\n",
    "    h_values=[]\n",
    "\n",
    "    a = self.W[0].T @ X + self.b[0]\n",
    "    hidden_layer_input = self.activate_hidden(a)\n",
    "    \n",
    "    a_values.append(a)\n",
    "    h_values.append(hidden_layer_input)\n",
    "\n",
    "    for i in range(1,self.num_hidden_layer):\n",
    "      a=self.W[i].T @ hidden_layer_input + self.b[i]\n",
    "      hidden_layer_output=self.activate_hidden(a)\n",
    "      hidden_layer_input = hidden_layer_output\n",
    "      a_values.append(a)\n",
    "      h_values.append(hidden_layer_input)\n",
    "\n",
    "    a=self.W[self.num_hidden_layer].T @ hidden_layer_input + self.b[self.num_hidden_layer]\n",
    "    output = self.activate_output(a)\n",
    "    a_values.append(a)\n",
    "    h_values.append(output)\n",
    "\n",
    "    return a_values,h_values\n",
    "\n",
    "  def backpropogation(self, X, Y, a_values, h_values):\n",
    "    size = len(h_values)\n",
    "    data_size = Y.shape[0]\n",
    "    delta_ak = self.lossDerivative(h_values[size - 1],Y)\n",
    "    delta_W=[]\n",
    "    delta_b=[]\n",
    "\n",
    "    for k in range(size - 1,0,-1):\n",
    "      delta_wk = h_values[k-1] @ delta_ak.T\n",
    "      delta_bk = np.sum(delta_ak,axis=1)\n",
    "      delta_W.append(delta_wk/data_size)\n",
    "      delta_b.append(delta_bk.reshape(delta_bk.shape[0],1)/data_size)\n",
    "\n",
    "      delta_hk = self.W[k] @ delta_ak\n",
    "      # print(delta_hk.shape)\n",
    "      # print(self.activation_function.sigmoidDerivative(a_values[k-1]).shape)\n",
    "      delta_ak = np.multiply(self.activate_hidden_derivative(a_values[k-1]),delta_hk)\n",
    "\n",
    "    delta_wk = X @ delta_ak.T\n",
    "    delta_bk = np.sum(delta_ak,axis=1)\n",
    "    delta_W.append(delta_wk/data_size)\n",
    "    delta_b.append(delta_bk.reshape(delta_bk.shape[0],1)/data_size)\n",
    "\n",
    "    return delta_W,delta_b\n",
    "  \n",
    "  def training(self, X, Y, X_val, Y_val ,epochs = 10 , weight_decay = 0 ,optimizer_name=\"gd\", lr=0.01, batch_size=32,parameters=[]):\n",
    "    optimize = Optimizer(optimizer_name).optimize()\n",
    "    \n",
    "    num_data = X.shape[1]\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Random shuffling of data\n",
    "    indexes_for_batch = np.arange(num_data)\n",
    "    np.random.shuffle(indexes_for_batch)\n",
    "\n",
    "    return optimize(self, X, Y, X_val, Y_val , lr, epochs, batch_size, indexes_for_batch, weight_decay=weight_decay)\n",
    "#     print(f'train accuracy: {self.calculateAccuracy(X,Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "wsFVNClaZ_Vs"
   },
   "outputs": [],
   "source": [
    "# # Split data to train and validation (10% of data)\n",
    "num_train_samples = 60000\n",
    "num_validate_samples = num_train_samples//10\n",
    "num_train_samples -= num_validate_samples\n",
    "\n",
    "X_valid = X_train[:num_validate_samples,:].reshape(num_validate_samples,num_size).T /255.0\n",
    "Y_valid = Y_train[:num_validate_samples]\n",
    "\n",
    "X = X_train[num_validate_samples:,:].reshape(num_train_samples,num_size).T / 255.0\n",
    "Y = Y_train[num_validate_samples:]\n",
    "\n",
    "def train():\n",
    "\n",
    "  wandb.init()\n",
    "\n",
    "  num_hidden_layer = wandb.config.num_hidden_layer\n",
    "  hidden_layer_size = np.full(num_hidden_layer,wandb.config.hidden_size)\n",
    "  hidden_layer_activation = wandb.config.activation\n",
    "  weight_name = wandb.config.weight_init\n",
    "  epochs = wandb.config.epochs\n",
    "  weight_decay = wandb.config.weight_decay\n",
    "  optimizer_name = wandb.config.optimizer\n",
    "  lr = wandb.config.lr\n",
    "  batch_size = wandb.config.batch_size\n",
    "\n",
    "  wandb.run.name = f'hln_{num_hidden_layer}_hls_{wandb.config.hidden_size}_hla_{hidden_layer_activation}_winit_{weight_name}_ep_{epochs}_op_{optimizer_name}_lr_{lr}_bs_{batch_size}_wd_{weight_decay}'\n",
    "\n",
    "#   loss = Loss('cross_entropy').findLoss()\n",
    "  nn = NN(num_samples = num_train_samples, num_hidden_layer = num_hidden_layer, hidden_layer_size = hidden_layer_size, hidden_layer_activation = hidden_layer_activation, weight_name = weight_name,loss_name = 'cross_entropy')\n",
    "  val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list = nn.training(X, Y, X_valid, Y_valid, epochs = epochs, weight_decay = weight_decay, optimizer_name = optimizer_name,lr = lr, batch_size = batch_size)\n",
    "\n",
    "\n",
    "  # y_val_predict = nn.feedforward(X_valid)\n",
    "  # y_train_predict = nn.feedforward(X)\n",
    "\n",
    "  # validation_loss = loss.crossEntropy(y_val_predict,Y_valid)\n",
    "  # training_loss = loss.crossEntropy(y_train_predict,Y)\n",
    "\n",
    "  # validation_accuracy = nn.calculateAccuracy(X_valid, Y_valid)\n",
    "  # training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "  print(val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list)\n",
    "  for i in range(len(val_loss_list)):\n",
    "    wandb.log({'validation_loss': val_loss_list[i],\n",
    "              'training_loss': train_loss_list[i],\n",
    "              'validation_accuracy': val_accuracy_list[i],\n",
    "              'training_accuracy': train_accuracy_list[i]\n",
    "              })\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550,
     "referenced_widgets": [
      "abc15c3f641546ecbc90e8bdd6a941ce",
      "910c5f78e0254062a3e7f269e454b8e2",
      "2e74356e6ea94b51bf8c039da82c1994",
      "d95345eee8bd4cfe81d10819f39f63a0",
      "1b794ec70f6645968425eb6d27db449d",
      "368929f2d8d84b789ca7ef2e584c638f",
      "7fef93c2a6184e93aee29efe40d9ebd6",
      "1afc886b32764c6d9d9d17eab6391c4b"
     ]
    },
    "id": "BaV_JvwppDeV",
    "outputId": "79f1a3ab-a8dd-4701-ca8f-38f59cf417e3"
   },
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'dl-project-final-bayes',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'validation_loss'\n",
    "        },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10]},\n",
    "        'num_hidden_layer':{'values' : [3, 4, 5]},\n",
    "        'hidden_size': {'values' : [32, 64, 128]},\n",
    "        'weight_decay': {'values' : [0, 0.0005, 0.5]},\n",
    "        'lr': {'values' : [1e-3, 1e-4]},\n",
    "        'optimizer':{'values' : ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']},\n",
    "        'weight_init':{'values' : ['random','xavier']},\n",
    "        'activation' : {'values' : ['sigmoid','tanh','relu']},\n",
    "     }\n",
    "}\n",
    "\n",
    "wandb.login(key = 'c425b887e2c725018a7f3a772582610fa54ef52c')\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='dl-project-final-bayes')\n",
    "wandb.agent(sweep_id, function=train, count=10000)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMaTOGnTMt7U",
    "outputId": "d6153e4e-df6d-4e57-a2a2-517f71702883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 => loss = 27.67735508725729\n",
      "epoch: 1 => loss = 27.68100139699475\n",
      "epoch: 2 => loss = 27.682903149675568\n",
      "epoch: 3 => loss = 27.68421797696125\n",
      "epoch: 4 => loss = 27.685219594678433\n",
      "epoch: 5 => loss = 27.68609092942811\n",
      "epoch: 6 => loss = 27.68685489962963\n",
      "epoch: 7 => loss = 27.68750790085528\n",
      "epoch: 8 => loss = 27.688070732704702\n",
      "epoch: 9 => loss = 27.688565917261357\n",
      "training loss : 27.688565917261357 \n",
      "training accuracy : 89.79074074074074\n",
      "tesing loss : 27.68377534383738 \n",
      "testing accuracy : 86.69 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "num_train_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "num_validate_samples = num_train_samples//10\n",
    "num_train_samples -= num_validate_samples\n",
    "\n",
    "X_valid = X_train[:num_validate_samples,:].reshape(num_validate_samples,input_size).T /255.0\n",
    "Y_valid = Y_train[:num_validate_samples]\n",
    "\n",
    "X = X_train[num_validate_samples:,:].reshape(num_train_samples,input_size).T / 255.0\n",
    "Y = Y_train[num_validate_samples:]\n",
    "\n",
    "X_test = X_test.reshape(num_test_samples,input_size).T / 255.0\n",
    "Y_test = Y_test\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "activation = 'tanh'\n",
    "batch_size=16\n",
    "epochs=10\n",
    "hidden_size= 128\n",
    "lr= 0.001\n",
    "num_hidden_layer= 5\n",
    "optimizer='momentum'\n",
    "weight_decay=0\n",
    "weight_init='xavier'\n",
    "loss_name = 'mean_squared_error'\n",
    "\n",
    "# loss = Loss('cross_entropy').findLoss()\n",
    "nn = NN(num_samples = num_train_samples, num_hidden_layer = num_hidden_layer, hidden_layer_size = np.full(num_hidden_layer,hidden_size), hidden_layer_activation = activation, weight_name = weight_init,loss_name=loss_name)\n",
    "val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list = nn.training(X, Y,X_valid,Y_valid, epochs = epochs, weight_decay = weight_decay, optimizer_name = optimizer,lr = lr, batch_size = batch_size)\n",
    "\n",
    "test_predict = nn.feedforward(X_test)\n",
    "test_predict_label_format = np.argmax(test_predict,axis=0)\n",
    "test_loss = nn.loss(test_predict,Y_test)\n",
    "test_accuracy = nn.calculateAccuracy(X_test, Y_test)\n",
    "\n",
    "list_length = len(val_loss_list)\n",
    "\n",
    "# wandb.login(key = 'c425b887e2c725018a7f3a772582610fa54ef52c')\n",
    "# wandb.init(project=\"temp\")\n",
    "# wandb.run.name = f'confusion matrix for training'\n",
    "# for i in range(list_length):\n",
    "#   wandb.log({'validation_loss': val_loss_list[i],\n",
    "#             'training_loss': train_loss_list[i],\n",
    "#             'validation_accuracy': val_accuracy_list[i],\n",
    "#             'training_accuracy': train_accuracy_list[i]\n",
    "#             })\n",
    "\n",
    "# wandb.log({'testing_loss': test_loss,\n",
    "#             'testing_accuracy': test_accuracy\n",
    "#             })\n",
    "\n",
    "\n",
    "# wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "#                         y_true=Y_test, preds=test_predict_label_format,class_names=class_names)})\n",
    "\n",
    "# wandb.finish()\n",
    "print(f'training loss : {train_loss_list[list_length-1]} \\ntraining accuracy : {train_accuracy_list[list_length-1]}\\ntesing loss : {test_loss} \\ntesting accuracy : {test_accuracy} \\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKTYedF4uTe89A1e55M08b",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1afc886b32764c6d9d9d17eab6391c4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b794ec70f6645968425eb6d27db449d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e74356e6ea94b51bf8c039da82c1994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fef93c2a6184e93aee29efe40d9ebd6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1afc886b32764c6d9d9d17eab6391c4b",
      "value": 0.0785171540409855
     }
    },
    "368929f2d8d84b789ca7ef2e584c638f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fef93c2a6184e93aee29efe40d9ebd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "910c5f78e0254062a3e7f269e454b8e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b794ec70f6645968425eb6d27db449d",
      "placeholder": "​",
      "style": "IPY_MODEL_368929f2d8d84b789ca7ef2e584c638f",
      "value": "0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "abc15c3f641546ecbc90e8bdd6a941ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_910c5f78e0254062a3e7f269e454b8e2",
       "IPY_MODEL_2e74356e6ea94b51bf8c039da82c1994"
      ],
      "layout": "IPY_MODEL_d95345eee8bd4cfe81d10819f39f63a0"
     }
    },
    "d95345eee8bd4cfe81d10819f39f63a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
