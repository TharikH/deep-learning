{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TharikH/deep-learning/blob/main/dl_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxC5XjBomovq",
    "outputId": "50f0bffd-fa97-432c-9fe0-0ca91acf5c5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 18:38:29.039310: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 18:38:29.587679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tharikh/plugin/ns-allinone-2.35/otcl-1.14:/home/tharikh/plugin/ns-allinone-2.35/lib\n",
      "2023-03-17 18:38:29.587703: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-17 18:38:30.608932: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tharikh/plugin/ns-allinone-2.35/otcl-1.14:/home/tharikh/plugin/ns-allinone-2.35/lib\n",
      "2023-03-17 18:38:30.609046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tharikh/plugin/ns-allinone-2.35/otcl-1.14:/home/tharikh/plugin/ns-allinone-2.35/lib\n",
      "2023-03-17 18:38:30.609053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# in colab uncomment below statement\n",
    "# !pip install wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import copy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uib1izEEoToJ",
    "outputId": "c11588ed-ee88-456b-a446-9a3b8d56ece4"
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fEFcyAeBo_D6"
   },
   "outputs": [],
   "source": [
    "num_train_samples = X_train.shape[0]\n",
    "num_size = X_train.shape[1] * X_train.shape[2]\n",
    "num_test_samples = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRTo19OU1L3p",
    "outputId": "f098b3cd-da4e-4c3d-ce8d-0c392ea85eef"
   },
   "outputs": [],
   "source": [
    "input_size=num_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdNI9lnQ2oIM"
   },
   "source": [
    "plot 1 sample image for each class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "AZtSRC16y5b5",
    "outputId": "cccbce4f-2590-4ace-e426-8e164330bcde"
   },
   "source": [
    "example_y = []\n",
    "example_x = []\n",
    "plt.figure(figsize=(10,10))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "k=0\n",
    "wandb.login(key = 'c425b887e2c725018a7f3a772582610fa54ef52c')\n",
    "\n",
    "for i in range(num_train_samples):\n",
    "  if Y_train[i] not in example_y:\n",
    "    example_y.append(Y_train[i])\n",
    "    example_x.append(X_train[i])\n",
    "    \n",
    "    wandb.init(project=\"dl-project-final\")\n",
    "    wandb.run.name = f'{class_names[Y_train[i]]}'\n",
    "    wandb.log({\"examples\": [wandb.Image(X_train[i],caption=class_names[Y_train[i]])]})\n",
    "    \n",
    "    plt.subplot(5,5,k+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[Y_train[i]])\n",
    "    k+=1\n",
    "    if k == 10:\n",
    "      break\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OaupL4eL2QoO"
   },
   "outputs": [],
   "source": [
    "#parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TfjhpkZr-T5I"
   },
   "outputs": [],
   "source": [
    "#class having all required activations\n",
    "\n",
    "class Activations():\n",
    "  def __init__(self):\n",
    "    self.activation_dict={\n",
    "        \"sigmoid\":self.sigmoid,\n",
    "        \"softmax\":self.softmax,\n",
    "        \"tanh\":self.tanh,\n",
    "        \"relu\":self.relu,\n",
    "        \"identity\":self.identity\n",
    "    }\n",
    "    self.derivative_dict={\n",
    "        \"sigmoid\":self.sigmoidDerivative,\n",
    "        \"softmax\":self.softmaxDerivative,\n",
    "        \"tanh\":self.tanhDerivative,\n",
    "        \"relu\":self.reluDerivative,\n",
    "        \"identity\":self.identity\n",
    "    }\n",
    "\n",
    "  def activate(self, activation_function = \"sigmoid\"):\n",
    "    return self.activation_dict[activation_function]\n",
    "\n",
    "  def derivate(self,activation_function = \"sigmoid\"):\n",
    "    return self.derivative_dict[activation_function]\n",
    "\n",
    "  def sigmoid(self, x):\n",
    "    z = x.copy()\n",
    "    z[x < 0] = np.exp(x[x < 0])/(1 + np.exp(x[x<0]))\n",
    "    z[x >= 0] = 1/(1+np.exp(-x[x >= 0]))\n",
    "    return z\n",
    "\n",
    "  def softmax(self, x):\n",
    "    max_element = np.max(x,axis=0)\n",
    "    z = np.exp(x - max_element)\n",
    "    total = sum(z)\n",
    "    z = z/total\n",
    "    return z\n",
    "  \n",
    "  def tanh(self, x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "  def identity(self,x):\n",
    "    return x\n",
    "\n",
    "  def identityDerivative(self, x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "  def tanhDerivative(self, x):\n",
    "    z = self.tanh(x)\n",
    "    return 1 - z**2\n",
    "\n",
    "  def softmaxDerivative(self,x):\n",
    "    pass\n",
    "  \n",
    "  def sigmoidDerivative(self,x):\n",
    "    z = self.sigmoid(x)\n",
    "    return  z*(1 - z)\n",
    "  \n",
    "  def relu(self,x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "  def reluDerivative(self,x):\n",
    "    z = x.copy()\n",
    "    z[x < 0]=0\n",
    "    z[x > 0]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "znaRtyTqZ-dT"
   },
   "outputs": [],
   "source": [
    "# class having all losses and its derivatives\n",
    "class Loss():\n",
    "  def __init__(self,loss_name):\n",
    "    self.loss_dict = {\n",
    "        'cross_entropy':self.crossEntropy,\n",
    "        'mean_squared_error':self.mse\n",
    "    }\n",
    "    self.loss_name = loss_name\n",
    "\n",
    "  def findLoss(self):\n",
    "    return self.loss_dict[self.loss_name]\n",
    "    \n",
    "  def crossEntropy(self ,Y_hat, Y, weight_decay=0,W=[]):\n",
    "    loss=0\n",
    "    num_samples = Y_hat.shape[1]\n",
    "    for i in range(num_samples):\n",
    "      loss+=np.log(Y_hat[:,i][Y[i]]  if Y_hat[:,i][Y[i]] != 0 else 1e-9)\n",
    "    \n",
    "    decay_loss = 0\n",
    "    for i in range(len(W)):\n",
    "        decay_loss+=np.sum(W[i] ** 2)\n",
    "\n",
    "\n",
    "    return (-loss + (weight_decay * decay_loss)/2)/num_samples\n",
    "\n",
    "  def crossEntropyDerivative(self,Y_hat, Y):\n",
    "    derivative = np.zeros(Y_hat.shape)\n",
    "    for i in range(Y_hat.shape[1]):\n",
    "      derivative[:,i][Y[i]] = 1/(Y_hat[:,i][Y[i]])\n",
    "    return derivative\n",
    "\n",
    "  def mse(self, Y_hat, Y):\n",
    "    m,n = Y_hat.shape\n",
    "    diff_matrix = (Y_hat - Y)**2\n",
    "    loss = np.sum(diff_matrix)\n",
    "    return loss/(m*n)\n",
    "\n",
    "  def mseDerivative(self, Y_hat , Y):\n",
    "    derivative = np.zeros(Y_hat.shape)\n",
    "    for i in range(Y_hat.shape[1]):\n",
    "      derivative[:,i][Y[i]] = 1/(Y_hat[:,i][Y[i]])\n",
    "    return derivative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tVu-t4PF6tiJ"
   },
   "outputs": [],
   "source": [
    "# class having all the weight initializations\n",
    "class WeightInit():\n",
    "  def __init__(self,weight_name):\n",
    "    self.weight_name = weight_name\n",
    "    self.weight_dict={\n",
    "        \"zero\":self.zeroInit,\n",
    "        \"random\":self.randomInit,\n",
    "        \"xavier\":self.xavierInit\n",
    "    }\n",
    "\n",
    "  def initializeWeight(self):\n",
    "    return self.weight_dict[self.weight_name]\n",
    "    \n",
    "  \n",
    "  def zeroInit(self, shape, flag=0):\n",
    "    x = np.zeros(shape)\n",
    "    return x\n",
    "\n",
    "  def randomInit(self, shape, flag=0):\n",
    "    x = np.random.normal(loc=0,scale=1,size=shape)\n",
    "    return x\n",
    "\n",
    "  \n",
    "  def xavierInit(self, shape, flag=0):\n",
    "    x = np.random.randn(*shape) * np.sqrt(2/shape[0]) if flag == 0 else self.zeroInit(shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V2gS7xaQ50Hd"
   },
   "outputs": [],
   "source": [
    "# classes having all the optimizers\n",
    "class Optimizer():\n",
    "  def __init__(self,optimizer_name=\"gd\"):\n",
    "    self.optimizer_name = optimizer_name\n",
    "    self.optimizer_dict={\n",
    "        \"gd\":self.gradient_descent,\n",
    "        \"sgd\":self.gradient_descent,\n",
    "        \"momentum\":self.momentum,\n",
    "        \"nag\":self.nesterov,\n",
    "        \"rmsprop\":self.rmsprop,\n",
    "        \"adam\" : self.adam,\n",
    "        \"nadam\" : self.nadam\n",
    "    }\n",
    "\n",
    "  def optimize(self):\n",
    "    return self.optimizer_dict[self.optimizer_name]\n",
    "    \n",
    "  \n",
    "  def gradient_descent(self, nn, X, Y, X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [], weight_decay = 0):\n",
    "\n",
    "    num_data = X.shape[1]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          nn.W[j] = nn.W[j] - lr * delta_W[nn.num_hidden_layer - j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - lr * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y,weight_decay,nn.W)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "\n",
    "  def momentum(self, nn, X, Y, X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    ut_w,ut_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          ut_w[j] = beta*ut_w[j] + delta_W[nn.num_hidden_layer - j]\n",
    "          ut_b[j] = beta*ut_b[j] + delta_b[nn.num_hidden_layer - j] \n",
    "\n",
    "          nn.W[j] = nn.W[j] - lr * ut_w[j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - lr * ut_b[j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "\n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "\n",
    "  def nesterov(self, nn, X, Y, X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    ut_w,ut_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "  \n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "\n",
    "        old_W = copy.deepcopy(nn.W)\n",
    "        old_b = copy.deepcopy(nn.b)\n",
    "        \n",
    "        for k in range(nn.num_hidden_layer + 1):\n",
    "          nn.W[k] = nn.W[k] - beta *  ut_w[k]\n",
    "          nn.b[k] = nn.b[k] - beta *  ut_b[k]\n",
    "\n",
    "        \n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          ut_w[j] = beta*ut_w[j] + delta_W[nn.num_hidden_layer - j]\n",
    "          ut_b[j] = beta*ut_b[j] + delta_b[nn.num_hidden_layer - j] \n",
    "\n",
    "          nn.W[j] = old_W[j] - lr * ut_w[j] - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = old_b[j] - lr * ut_b[j]\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "\n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "\n",
    "\n",
    "  def rmsprop(self, nn, X, Y,X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9,0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    beta = parameters[0]\n",
    "    epsilon = parameters[1]\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          vt_w[j] = beta*vt_w[j] + (1 - beta) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta*vt_b[j] + (1 - beta) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(lr * delta_W[nn.num_hidden_layer - j],np.sqrt(vt_w[j] + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(lr * delta_b[nn.num_hidden_layer - j],np.sqrt(vt_b[j] + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "\n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def adam(self, nn, X, Y, X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9,0.99,0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    mt_w,mt_b = nn.initializeWeights(\"zero\")\n",
    "    beta1 = parameters[0]\n",
    "    beta2 = parameters[1]\n",
    "    epsilon = parameters[2]\n",
    "\n",
    "    t=0\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        t+=1\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          mt_w[j] = beta1 * mt_w[j] + (1 - beta1) * delta_W[nn.num_hidden_layer - j]\n",
    "          mt_b[j] = beta1 * mt_b[j] + (1 - beta1) * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "          mt_w_dash = mt_w[j] / (1 - beta1 ** t)\n",
    "          mt_b_dash = mt_b[j] / (1 - beta1 ** t)\n",
    "\n",
    "\n",
    "          vt_w[j] = beta2*vt_w[j] + (1 - beta2) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta2*vt_b[j] + (1 - beta2) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          vt_w_dash = vt_w[j] / (1 - beta2 ** t)\n",
    "          vt_b_dash = vt_b[j] / (1 - beta2 ** t)           \n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(lr * mt_w_dash,np.sqrt(vt_w_dash + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(lr * mt_b_dash,np.sqrt(vt_b_dash + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "\n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    \n",
    "  def nadam(self, nn, X, Y,X_val, Y_val, loss, lr, epochs, batch_size,indexes_for_batch,parameters = [0.9, 0.999, 0.1], weight_decay = 0):\n",
    "    num_data = X.shape[1]\n",
    "    vt_w,vt_b = nn.initializeWeights(\"zero\")\n",
    "    mt_w,mt_b = nn.initializeWeights(\"zero\")\n",
    "    beta1 = parameters[0]\n",
    "    beta2 = parameters[1]\n",
    "    epsilon = parameters[2]\n",
    "\n",
    "    t=0\n",
    "\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for batch in range(0,num_data,batch_size):\n",
    "        t+=1\n",
    "        X_batch = X[:,indexes_for_batch[batch:batch + batch_size]]\n",
    "        Y_batch = Y[indexes_for_batch[batch:batch + batch_size]]\n",
    "\n",
    "        # self.W,self.b = self.initializeWeights()\n",
    "        a_values,h_values = nn.forwardpropogation(X_batch)\n",
    "        delta_W, delta_b = nn.backpropogation(X_batch,Y_batch,a_values, h_values)\n",
    "        # print(np.sum(delta_W[0], axis = 0))\n",
    "        for j in range(nn.num_hidden_layer + 1):\n",
    "          mt_w[j] = beta1 * mt_w[j] + (1 - beta1) * delta_W[nn.num_hidden_layer - j]\n",
    "          mt_b[j] = beta1 * mt_b[j] + (1 - beta1) * delta_b[nn.num_hidden_layer - j]\n",
    "\n",
    "          mt_w_dash = mt_w[j] / (1 - beta1 ** t)\n",
    "          mt_b_dash = mt_b[j] / (1 - beta1 ** t)\n",
    "\n",
    "\n",
    "          vt_w[j] = beta2*vt_w[j] + (1 - beta2) * np.multiply(delta_W[nn.num_hidden_layer - j],delta_W[nn.num_hidden_layer - j]) \n",
    "          vt_b[j] = beta2*vt_b[j] + (1 - beta2) * np.multiply(delta_b[nn.num_hidden_layer - j],delta_b[nn.num_hidden_layer - j])\n",
    "\n",
    "          vt_w_dash = vt_w[j] / (1 - beta2 ** t)\n",
    "          vt_b_dash = vt_b[j] / (1 - beta2 ** t)           \n",
    "\n",
    "          w_update_numerator = lr * (beta1 * mt_w_dash + ((1 - beta1)* delta_W[nn.num_hidden_layer - j]/(1 - beta1 ** t)))\n",
    "          b_update_numerator = lr * (beta1 * mt_b_dash + ((1 - beta1)* delta_b[nn.num_hidden_layer - j]/(1 - beta1 ** t)))\n",
    "\n",
    "          nn.W[j] = nn.W[j] - np.divide(w_update_numerator,np.sqrt(vt_w_dash + epsilon)) - lr*weight_decay*nn.W[j]\n",
    "          nn.b[j] = nn.b[j] - np.divide(b_update_numerator,np.sqrt(vt_b_dash + epsilon))\n",
    "\n",
    "      Y_hat = nn.feedforward(X)\n",
    "      loss_value = loss(Y_hat,Y)\n",
    "      print(f\"epoch: {epoch} => loss = {loss_value}\")\n",
    "      y_val_predict = nn.feedforward(X_val)\n",
    "      y_train_predict = nn.feedforward(X)\n",
    "\n",
    "      validation_loss = loss(y_val_predict,Y_val,weight_decay,nn.W)\n",
    "      training_loss = loss(y_train_predict,Y,weight_decay,nn.W)\n",
    "\n",
    "      validation_accuracy = nn.calculateAccuracy(X_val, Y_val)\n",
    "      training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "\n",
    "      \n",
    "      val_loss_list.append(validation_loss)\n",
    "      val_accuracy_list.append(validation_accuracy)\n",
    "      train_loss_list.append(training_loss)\n",
    "      train_accuracy_list.append(training_accuracy)\n",
    "    \n",
    "    return val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5lWfym6w4BKL"
   },
   "outputs": [],
   "source": [
    "# Base class for all neural networks\n",
    "\n",
    "class NeuralNetwork():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def getParameters(self):\n",
    "    pass\n",
    "  def feedforward():\n",
    "    pass\n",
    "  def backpropogation():\n",
    "    pass\n",
    "  def test(self):\n",
    "    pass\n",
    "  def train(self):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EPQ7zGjg6gEk"
   },
   "outputs": [],
   "source": [
    "# Neural Network for this particular neural network\n",
    "\n",
    "class NN(NeuralNetwork):\n",
    "  def __init__(self, num_samples = 60000, input_size = 784, output_size = 10, num_hidden_layer = 3, hidden_layer_size=np.array([64,64,64]), data_name = \"Fashion_mnsit\", hidden_layer_activation=\"relu\", output_layer_activation=\"softmax\", weight_name=\"xavier\"):\n",
    "    self.num_samples = num_samples\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.num_hidden_layer = num_hidden_layer\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.W, self.b = self.initializeWeights(weight_name)\n",
    "    self.hidden_layer_activation = hidden_layer_activation\n",
    "    self.output_layer_activation = output_layer_activation\n",
    "    self.activation_function = Activations()\n",
    "    self.activate_hidden = self.activation_function.activate(hidden_layer_activation)\n",
    "    self.activate_hidden_derivative = self.activation_function.derivate(hidden_layer_activation)\n",
    "    self.activate_output = self.activation_function.activate(output_layer_activation)\n",
    "    self.parameters = {\n",
    "        \"data_name\":data_name,\n",
    "        \"num_samples\":num_samples,\n",
    "        \"input_size\":input_size,\n",
    "        \"output_size\":output_size,\n",
    "        \"num_hidden_layer\":num_hidden_layer,\n",
    "        \"hidden_layer_size\":hidden_layer_size,\n",
    "        \"hidden_layer_activation\":hidden_layer_activation,\n",
    "        \"output_layer_activation\":output_layer_activation,\n",
    "        \"weight_init\":weight_name\n",
    "    }\n",
    "\n",
    "  def getParameters(self):\n",
    "    return self.parameters\n",
    "\n",
    "  def initializeWeights(self, weight_name):\n",
    "    W = []\n",
    "    b= []\n",
    "    input_size = self.input_size\n",
    "    weight_init = WeightInit(weight_name).initializeWeight()\n",
    "    for i in range(self.num_hidden_layer):\n",
    "      output_size = self.hidden_layer_size[i]\n",
    "      W.append(weight_init((input_size, output_size ),0))\n",
    "      b.append(weight_init((output_size, 1 ),1))\n",
    "      input_size = output_size\n",
    "    \n",
    "    output_size = self.output_size\n",
    "\n",
    "    W.append(weight_init((input_size, output_size),0))\n",
    "    b.append(weight_init((output_size, 1),1))\n",
    "\n",
    "    return W, b\n",
    "\n",
    "  def calculateAccuracy(self, X, Y):\n",
    "    Y_hat = self.feedforward(X)\n",
    "    size = Y_hat.shape[1]\n",
    "    score=0\n",
    "    for i in range(size):\n",
    "      if(np.argmax(Y_hat[:,i]) ==  Y[i]):\n",
    "          score+=1\n",
    "\n",
    "    return score/size * 100\n",
    "\n",
    "  def findOneHotVector(self,Y_hat, Y):\n",
    "    vector = np.zeros(Y_hat.shape)\n",
    "    for i in range(Y_hat.shape[1]):\n",
    "      vector[:,i][Y[i]] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "  def feedforward(self, X):\n",
    "    a = self.W[0].T @ X + self.b[0]\n",
    "    hidden_layer_input = self.activate_hidden(a)\n",
    "\n",
    "    for i in range(1,self.num_hidden_layer):\n",
    "      a=self.W[i].T @ hidden_layer_input + self.b[i]\n",
    "      hidden_layer_output=self.activate_hidden(a)\n",
    "      hidden_layer_input = hidden_layer_output\n",
    "\n",
    "    a=self.W[self.num_hidden_layer].T @ hidden_layer_input + self.b[self.num_hidden_layer]\n",
    "    output = self.activate_output(a)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "  def forwardpropogation(self, X):\n",
    "    a_values=[]\n",
    "    h_values=[]\n",
    "\n",
    "    a = self.W[0].T @ X + self.b[0]\n",
    "    hidden_layer_input = self.activate_hidden(a)\n",
    "    \n",
    "    a_values.append(a)\n",
    "    h_values.append(hidden_layer_input)\n",
    "\n",
    "    for i in range(1,self.num_hidden_layer):\n",
    "      a=self.W[i].T @ hidden_layer_input + self.b[i]\n",
    "      hidden_layer_output=self.activate_hidden(a)\n",
    "      hidden_layer_input = hidden_layer_output\n",
    "      a_values.append(a)\n",
    "      h_values.append(hidden_layer_input)\n",
    "\n",
    "    a=self.W[self.num_hidden_layer].T @ hidden_layer_input + self.b[self.num_hidden_layer]\n",
    "    output = self.activate_output(a)\n",
    "    a_values.append(a)\n",
    "    h_values.append(output)\n",
    "\n",
    "    return a_values,h_values\n",
    "\n",
    "  def backpropogation(self, X, Y, a_values, h_values):\n",
    "    size = len(h_values)\n",
    "    data_size = Y.shape[0]\n",
    "    delta_ak = -(self.findOneHotVector(h_values[size - 1],Y) - h_values[size - 1])\n",
    "    delta_W=[]\n",
    "    delta_b=[]\n",
    "\n",
    "    for k in range(size - 1,0,-1):\n",
    "      delta_wk = h_values[k-1] @ delta_ak.T\n",
    "      delta_bk = np.sum(delta_ak,axis=1)\n",
    "      delta_W.append(delta_wk/data_size)\n",
    "      delta_b.append(delta_bk.reshape(delta_bk.shape[0],1)/data_size)\n",
    "\n",
    "      delta_hk = self.W[k] @ delta_ak\n",
    "      # print(delta_hk.shape)\n",
    "      # print(self.activation_function.sigmoidDerivative(a_values[k-1]).shape)\n",
    "      delta_ak = np.multiply(self.activate_hidden_derivative(a_values[k-1]),delta_hk)\n",
    "\n",
    "    delta_wk = X @ delta_ak.T\n",
    "    delta_bk = np.sum(delta_ak,axis=1)\n",
    "    delta_W.append(delta_wk/data_size)\n",
    "    delta_b.append(delta_bk.reshape(delta_bk.shape[0],1)/data_size)\n",
    "\n",
    "    return delta_W,delta_b\n",
    "  \n",
    "  def training(self, X, Y, X_val, Y_val,loss ,epochs = 10 , weight_decay = 0 ,optimizer_name=\"gd\", lr=0.01, batch_size=32,parameters=[]):\n",
    "    optimize = Optimizer(optimizer_name).optimize()\n",
    "    \n",
    "    num_data = X.shape[1]\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Random shuffling of data\n",
    "    indexes_for_batch = np.arange(num_data)\n",
    "    np.random.shuffle(indexes_for_batch)\n",
    "\n",
    "    return optimize(self, X, Y, X_val, Y_val ,loss, lr, epochs, batch_size, indexes_for_batch, weight_decay=weight_decay)\n",
    "#     print(f'train accuracy: {self.calculateAccuracy(X,Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wsFVNClaZ_Vs"
   },
   "outputs": [],
   "source": [
    "# # Split data to train and validation (10% of data)\n",
    "num_train_samples = 60000\n",
    "num_validate_samples = num_train_samples//10\n",
    "num_train_samples -= num_validate_samples\n",
    "\n",
    "X_valid = X_train[:num_validate_samples,:].reshape(num_validate_samples,num_size).T /255.0\n",
    "Y_valid = Y_train[:num_validate_samples]\n",
    "\n",
    "X = X_train[num_validate_samples:,:].reshape(num_train_samples,num_size).T / 255.0\n",
    "Y = Y_train[num_validate_samples:]\n",
    "\n",
    "def train():\n",
    "\n",
    "  wandb.init()\n",
    "\n",
    "  num_hidden_layer = wandb.config.num_hidden_layer\n",
    "  hidden_layer_size = np.full(num_hidden_layer,wandb.config.hidden_size)\n",
    "  hidden_layer_activation = wandb.config.activation\n",
    "  weight_name = wandb.config.weight_init\n",
    "  epochs = wandb.config.epochs\n",
    "  weight_decay = wandb.config.weight_decay\n",
    "  optimizer_name = wandb.config.optimizer\n",
    "  lr = wandb.config.lr\n",
    "  batch_size = wandb.config.batch_size\n",
    "\n",
    "  wandb.run.name = f'hln_{num_hidden_layer}_hls_{wandb.config.hidden_size}_hla_{hidden_layer_activation}_winit_{weight_name}_ep_{epochs}_op_{optimizer_name}_lr_{lr}_bs_{batch_size}_wd_{weight_decay}'\n",
    "\n",
    "  loss = Loss('cross_entropy').findLoss()\n",
    "  nn = NN(num_samples = num_train_samples, num_hidden_layer = num_hidden_layer, hidden_layer_size = hidden_layer_size, hidden_layer_activation = hidden_layer_activation, weight_name = weight_name)\n",
    "  val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list = nn.training(X, Y, X_valid, Y_valid, loss = loss, epochs = epochs, weight_decay = weight_decay, optimizer_name = optimizer_name,lr = lr, batch_size = batch_size)\n",
    "\n",
    "\n",
    "  # y_val_predict = nn.feedforward(X_valid)\n",
    "  # y_train_predict = nn.feedforward(X)\n",
    "\n",
    "  # validation_loss = loss.crossEntropy(y_val_predict,Y_valid)\n",
    "  # training_loss = loss.crossEntropy(y_train_predict,Y)\n",
    "\n",
    "  # validation_accuracy = nn.calculateAccuracy(X_valid, Y_valid)\n",
    "  # training_accuracy = nn.calculateAccuracy(X, Y)\n",
    "  print(val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list)\n",
    "  for i in range(len(val_loss_list)):\n",
    "    wandb.log({'validation_loss': val_loss_list[i],\n",
    "              'training_loss': train_loss_list[i],\n",
    "              'validation_accuracy': val_accuracy_list[i],\n",
    "              'training_accuracy': train_accuracy_list[i]\n",
    "              })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550,
     "referenced_widgets": [
      "abc15c3f641546ecbc90e8bdd6a941ce",
      "910c5f78e0254062a3e7f269e454b8e2",
      "2e74356e6ea94b51bf8c039da82c1994",
      "d95345eee8bd4cfe81d10819f39f63a0",
      "1b794ec70f6645968425eb6d27db449d",
      "368929f2d8d84b789ca7ef2e584c638f",
      "7fef93c2a6184e93aee29efe40d9ebd6",
      "1afc886b32764c6d9d9d17eab6391c4b"
     ]
    },
    "id": "BaV_JvwppDeV",
    "outputId": "79f1a3ab-a8dd-4701-ca8f-38f59cf417e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m058\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/tharikh/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ozia448t\n",
      "Sweep URL: https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3f1n4d0o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tharikh/dl-ass-1/wandb/run-20230317_183913-3f1n4d0o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs22m058/dl-project-final-bayes/runs/3f1n4d0o' target=\"_blank\">worthy-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m058/dl-project-final-bayes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/runs/3f1n4d0o' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/runs/3f1n4d0o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 => loss = 2.271754610481811\n",
      "epoch: 1 => loss = 2.24475774616205\n",
      "epoch: 2 => loss = 2.2754848945566892\n",
      "epoch: 3 => loss = 2.2947781980663184\n",
      "epoch: 4 => loss = 2.301841007881296\n",
      "[2.767275593041437, 2.457553383804704, 2.3665326400718656, 2.333953215014794, 2.3189146521168493] [16.716666666666665, 18.516666666666666, 15.75, 11.65, 9.333333333333334] [2.326030962633175, 2.26809321864309, 2.285519194653081, 2.299093219597299, 2.303696725648604] [17.014814814814816, 19.383333333333333, 16.294444444444444, 12.205555555555556, 10.074074074074074]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f475abea06646b6bef638b1f14aced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy</td><td>▆█▆▃▁</td></tr><tr><td>training_loss</td><td>█▁▃▅▅</td></tr><tr><td>validation_accuracy</td><td>▇█▆▃▁</td></tr><tr><td>validation_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy</td><td>10.07407</td></tr><tr><td>training_loss</td><td>2.3037</td></tr><tr><td>validation_accuracy</td><td>9.33333</td></tr><tr><td>validation_loss</td><td>2.31891</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-sweep-1</strong> at: <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/runs/3f1n4d0o' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/runs/3f1n4d0o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230317_183913-3f1n4d0o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 55430hyh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dfa05a3b134de09498a86222932d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666938148337067, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tharikh/dl-ass-1/wandb/run-20230317_183946-55430hyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs22m058/dl-project-final-bayes/runs/55430hyh' target=\"_blank\">solar-sweep-2</a></strong> to <a href='https://wandb.ai/cs22m058/dl-project-final-bayes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/sweeps/ozia448t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs22m058/dl-project-final-bayes/runs/55430hyh' target=\"_blank\">https://wandb.ai/cs22m058/dl-project-final-bayes/runs/55430hyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'dl-project-final-bayes',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'validation_loss'\n",
    "        },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10]},\n",
    "        'num_hidden_layer':{'values' : [3, 4, 5]},\n",
    "        'hidden_size': {'values' : [32, 64, 128]},\n",
    "        'weight_decay': {'values' : [0, 0.0005, 0.5]},\n",
    "        'lr': {'values' : [1e-3, 1e-4]},\n",
    "        'optimizer':{'values' : ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
    "        'weight_init':{'values' : ['random','xavier']},\n",
    "        'activation' : {'values' : ['sigmoid','tanh','relu']},\n",
    "     }\n",
    "}\n",
    "\n",
    "wandb.login(key = 'c425b887e2c725018a7f3a772582610fa54ef52c')\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='dl-project-final-bayes')\n",
    "wandb.agent(sweep_id, function=train, count=10000)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMaTOGnTMt7U",
    "outputId": "d6153e4e-df6d-4e57-a2a2-517f71702883"
   },
   "outputs": [],
   "source": [
    "num_train_samples = 60000\n",
    "num_validate_samples = num_train_samples//10\n",
    "num_train_samples -= num_validate_samples\n",
    "\n",
    "X_valid = X_train[:num_validate_samples,:].reshape(num_validate_samples,num_size).T /255.0\n",
    "Y_valid = Y_train[:num_validate_samples]\n",
    "\n",
    "X = X_train[num_validate_samples:,:].reshape(num_train_samples,num_size).T / 255.0\n",
    "Y = Y_train[num_validate_samples:]\n",
    "\n",
    "X_test = X_test.reshape(num_test_samples,input_size).T / 255.0\n",
    "Y_test = Y_test\n",
    "\n",
    "activation = 'tanh'\n",
    "batch_size=16\n",
    "epochs=10\n",
    "hidden_size= 128\n",
    "lr= 0.001\n",
    "num_hidden_layer= 5\n",
    "optimizer='momentum'\n",
    "weight_decay=0\n",
    "weight_init='xavier'\n",
    "\n",
    "# nn = NN(hidden_layer_size=np.array([64,64,64,64]),num_hidden_layer=4)\n",
    "# num_train_samples = 60000\n",
    "# X = X_train[:num_train_samples,:].reshape(num_train_samples,num_size).T /255\n",
    "# Y = Y_train[:num_train_samples]\n",
    "# print(X.shape)\n",
    "# nn.training(X,Y,batch_size=64,lr=0.0001,epochs=5,optimizer_name=\"nesterov\");\n",
    "\n",
    "\n",
    "nn = NN(num_samples = num_train_samples, num_hidden_layer = num_hidden_layer, hidden_layer_size = np.full(num_hidden_layer,hidden_size), hidden_layer_activation = activation, weight_name = weight_init)\n",
    "val_loss_list,val_accuracy_list,train_loss_list,train_accuracy_list = nn.training(X, Y,X_valid,Y_valid, epochs = epochs, weight_decay = weight_decay, optimizer_name = optimizer,lr = lr, batch_size = batch_size)\n",
    "\n",
    "test_predict = nn.feedforward(X_test)\n",
    "test_loss = loss(test_predict,Y_test)\n",
    "test_accuracy = nn.calculateAccuracy(X_test, Y_test)\n",
    "\n",
    "list_length = len(val_loss_list)\n",
    "# for i in range(list_length):\n",
    "#   wandb.log({'validation_loss': val_loss_list[i],\n",
    "#             'training_loss': train_loss_list[i],\n",
    "#             'validation_accuracy': val_accuracy_list[i],\n",
    "#             'training_accuracy': train_accuracy_list[i]\n",
    "#             })\n",
    "\n",
    "# wandb.log({'testing_loss': test_loss,\n",
    "#             'testing_accuracy': test_accuracy\n",
    "#             })\n",
    "\n",
    "# wandb.finish()\n",
    "\n",
    "print(f'training loss : {train_loss_list[list_length-1]} \\ntraining accuracy : {train_accuracy_list[list_length-1]}\\ntesing loss : {test_loss} \\ntesting accuracy : {test_accuracy} \\n ')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKTYedF4uTe89A1e55M08b",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1afc886b32764c6d9d9d17eab6391c4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b794ec70f6645968425eb6d27db449d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e74356e6ea94b51bf8c039da82c1994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fef93c2a6184e93aee29efe40d9ebd6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1afc886b32764c6d9d9d17eab6391c4b",
      "value": 0.0785171540409855
     }
    },
    "368929f2d8d84b789ca7ef2e584c638f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fef93c2a6184e93aee29efe40d9ebd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "910c5f78e0254062a3e7f269e454b8e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b794ec70f6645968425eb6d27db449d",
      "placeholder": "​",
      "style": "IPY_MODEL_368929f2d8d84b789ca7ef2e584c638f",
      "value": "0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "abc15c3f641546ecbc90e8bdd6a941ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_910c5f78e0254062a3e7f269e454b8e2",
       "IPY_MODEL_2e74356e6ea94b51bf8c039da82c1994"
      ],
      "layout": "IPY_MODEL_d95345eee8bd4cfe81d10819f39f63a0"
     }
    },
    "d95345eee8bd4cfe81d10819f39f63a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
